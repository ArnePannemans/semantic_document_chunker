python -m src.inference \
    --text "Machine learning has revolutionized the field of natural language processing over the past decade. The introduction of transformer architectures in 2017 marked a significant turning point in how we approach language understanding tasks. These models, based on attention mechanisms, have proven to be highly effective at capturing long-range dependencies in text. The original transformer paper, titled 'Attention Is All You Need,' proposed a novel architecture that dispensed with recurrence and convolution entirely. Instead, it relied solely on self-attention mechanisms to compute representations of input sequences. This approach allowed for much greater parallelization during training, leading to faster model development cycles and the ability to train on larger datasets. The key innovation was the multi-head attention mechanism, which allows the model to jointly attend to information from different representation subspaces at different positions. Following the success of transformers, several landmark models emerged that pushed the boundaries of what was possible with language AI. BERT, introduced by Google in 2018, demonstrated the power of bidirectional pre-training for language understanding. By training on a masked language modeling objective, BERT learned rich contextual representations that could be fine-tuned for a wide variety of downstream tasks. This pre-training and fine-tuning paradigm became the standard approach in NLP. Around the same time, OpenAI released GPT, which took a different approach by focusing on autoregressive language modeling. GPT was trained to predict the next token in a sequence, learning to generate coherent text in the process. Subsequent versions like GPT-2 and GPT-3 scaled up this approach dramatically, demonstrating that language models could exhibit impressive capabilities when trained on massive amounts of text data. The trend toward larger models has continued, with models now containing hundreds of billions of parameters. However, this scaling has raised important questions about efficiency, accessibility, and environmental impact. Training these massive models requires substantial computational resources and energy consumption. This has led to increased interest in more efficient architectures and training methods. Researchers are exploring techniques like model distillation, pruning, and quantization to make large language models more practical for deployment. Another active area of research is improving the sample efficiency of these models, reducing the amount of training data needed to achieve good performance. Despite these challenges, the impact of transformer-based language models on practical applications has been profound. They have enabled significant advances in machine translation, enabling more accurate and fluent translations across a wide variety of language pairs. In question answering systems, these models can now understand complex queries and retrieve relevant information from large knowledge bases. Summarization systems powered by transformers can generate concise summaries of long documents while preserving key information. The technology has also found applications beyond traditional NLP tasks. In code generation, models like GitHub Copilot assist developers by suggesting code completions and even entire functions. In scientific research, language models are being used to analyze literature, generate hypotheses, and assist in experimental design. The versatility of these models stems from their ability to learn general-purpose representations that can be adapted to many different tasks." \
    --adapter models/qwen3-14-recreation-run-v1/checkpoint-140